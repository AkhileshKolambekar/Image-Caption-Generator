{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, train_CNN = False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.train_CNN = train_CNN\n",
    "        \n",
    "        self.inception = models.inception_v3(pretrained = True, aux_logits = True)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)[0]\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings),dim = 0)\n",
    "        hiddens,_ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        \n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "    def forward(self, images ,captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features,captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length = 38):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                \n",
    "                hidden, states = self.decoderRNN.lstm(x,states)\n",
    "                output = self.decoderRNN.linear(hidden.squeeze(0))\n",
    "                predicted = torch.argmax(output)\n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0) # Prepare the output to input\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == '<EOS>':\n",
    "                    break\n",
    "            \n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0:'<PAD>',1:'<SOS>',2:'<EOS>',3:'<UNK>'}\n",
    "        self.stoi = {'<PAD>':0,'<SOS>':1,'<EOS>':2,'<UNK>':3}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def tokenize_caption(self,text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize_caption(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold and len(word)>1 and word.isalpha():\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    \n",
    "    def convert_to_vector(self, text):\n",
    "        tokenized_text = self.tokenize_caption(text)\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi['<UNK>'] for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, captions_file, transform = None, freq_threshold = 10):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.df = self.df[:4000]\n",
    "        \n",
    "        self.imgs = self.df['image']\n",
    "        self.captions = self.df['caption']\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir,img_id)).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        caption_vector = [self.vocab.stoi['<SOS>']]\n",
    "        caption_vector += self.vocab.convert_to_vector(caption)\n",
    "        caption_vector.append(self.vocab.stoi['<EOS>'])\n",
    "        \n",
    "        return img, torch.tensor(caption_vector)\n",
    "\n",
    "class MyCollate:\n",
    "    \n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs,dim = 0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first = False, padding_value = self.pad_idx)\n",
    "        return imgs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'D:/Dataset/flickr8k/images'\n",
    "\n",
    "dataset = FlickrDataset(root_dir = dir_path, captions_file = 'D:/Dataset/flickr8k/captions.txt', transform=transform)\n",
    "\n",
    "pad_idx = dataset.vocab.stoi['<PAD>']\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = torch.load('entire_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EXAMPLE 1 ##\n",
      "CORRECT: Child holding a red frisbee outdoors\n",
      "OUTPUT: <SOS> <UNK> little girl in <UNK> <UNK> <UNK> runs through the sand at the beach <EOS>\n",
      "\n",
      "## EXAMPLE 2 ##\n",
      "CORRECT: A small boat in the ocean\n",
      "OUTPUT: <SOS> <UNK> man <UNK> <UNK> <UNK> while <UNK> <UNK> <EOS>\n",
      "\n",
      "## EXAMPLE 3 ##\n",
      "CORRECT: Dog on a beach by the ocean\n",
      "OUTPUT: <SOS> <UNK> brown dog <UNK> <UNK> black and white dog <UNK> and <UNK> black dog are all standing in <UNK> field <UNK> <EOS>\n"
     ]
    }
   ],
   "source": [
    "new_model.eval()\n",
    "\n",
    "print('## EXAMPLE 1 ##')\n",
    "test_img1 = transform(Image.open(\"child.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
    "\n",
    "print(\"CORRECT: Child holding a red frisbee outdoors\")\n",
    "print(\n",
    "    \"OUTPUT: \"\n",
    "    + \" \".join(new_model.caption_image(test_img1.to(device), dataset.vocab))\n",
    ")\n",
    "\n",
    "print()\n",
    "print('## EXAMPLE 2 ##')\n",
    "test_img1 = transform(Image.open(\"boat.png\").convert(\"RGB\")).unsqueeze(0)\n",
    "\n",
    "print(\"CORRECT: A small boat in the ocean\")\n",
    "print(\n",
    "    \"OUTPUT: \"\n",
    "    + \" \".join(new_model.caption_image(test_img1.to(device), dataset.vocab))\n",
    ")\n",
    "\n",
    "print()\n",
    "print('## EXAMPLE 3 ##')\n",
    "test_img1 = transform(Image.open(\"dog.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
    "\n",
    "print(\"CORRECT: Dog on a beach by the ocean\")\n",
    "print(\n",
    "    \"OUTPUT: \"\n",
    "    + \" \".join(new_model.caption_image(test_img1.to(device), dataset.vocab))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cude'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
